# This frontend-oidc.toml config file will start a reva service that:
# - serves as the entypoint for legacy owncloud requests
# - serves http endpoints on port 20080
#   - /owncloud - ocdav
#   - /ocs - ocs
#   - /oauth2 - oidcprovider
#   - /.well-known - wellknown service to announce openid-configuration
#   - TODO ocm
# - authenticates requests using oidc bearer auth
# - serves the grpc services on port 18002
[core]
max_cpus = "2"

[log]
level = "debug"

[grpc]
address = "0.0.0.0:20099"
enabled_services = [
    "authprovider"
]

[grpc.services.authprovider]
auth_manager = "oidc"

[grpc.services.authprovider.auth_managers.oidc]
# If you want to use your own openid provider change this config
provider = "http://localhost:20080"
insecure = true
# credentials used for the introspection endpoint with basic auth
# also rate limit the endpoint: https://tools.ietf.org/html/rfc7662#section-4
# TODO(jfd) introduce rate limits
client_id = "reva"
client_secret = "foobar"

[grpc.interceptors.auth]
token_manager = "jwt"

[grpc.interceptors.auth.token_managers.jwt]
secret = "Pive-Fumkiu4"

[http]
#enabled_services = ["ocdav", "ocs"] # use this if you want to use your own oidc provider. 
enabled_services = ["wellknown", "oidcprovider", "ocdav", "ocs"]
enabled_middlewares = ["cors", "auth"]
address = "0.0.0.0:20080"

[http.middlewares.auth]
gateway = "localhost:19000"
credential_chain = ["basic", "bearer"]
token_strategy = "header"
token_writer = "header"
token_manager = "jwt"
skip_methods = [
    "/owncloud/status.php",
	"/oauth2",
	"/oauth2/auth", 
	"/oauth2/token",
	# TODO protect the introspection endpoint from external requests.
	# should only be reachable by internal services, which is why the
	# oidc-provider.toml has clientid and secret that are used for a basic auth
	"/oauth2/introspect",
	"/oauth2/userinfo",
	"/oauth2/sessions",
	"/.well-known/openid-configuration",
]

[http.middlewares.auth.token_managers.jwt]
secret = "Pive-Fumkiu4"

[http.middlewares.cors]
allowed_origins = ["*"]
allowed_methods = ["OPTIONS", "GET", "PUT", "POST", "DELETE", "MKCOL", "PROPFIND", "PROPPATCH", "MOVE", "COPY", "REPORT", "SEARCH"]
allowed_headers = ["Origin", "Accept", "Depth", "Content-Type", "X-Requested-With", "Authorization", "Ocs-Apirequest", "If-None-Match"]
allow_credentials = true
options_passthrough = false

[http.services.wellknown]
issuer = "http://localhost:20080"
authorization_endpoint = "http://localhost:20080/oauth2/auth"
token_endpoint = "http://localhost:20080/oauth2/token" 
#jwks_uri = ""
revocation_endpoint = "http://localhost:20080/oauth2/auth"
introspection_endpoint = "http://localhost:20080/oauth2/introspect"
userinfo_endpoint = "http://localhost:20080/oauth2/userinfo"
#end_session_endpoint = 

[http.services.oidcprovider]
prefix = "oauth2"
gateway = "localhost:19000"
issuer = "http://localhost:20080"

[http.services.oidcprovider.clients.phoenix]
id = "phoenix"
redirect_uris = ["http://localhost:8300/oidc-callback.html", "http://localhost:8300/"]
grant_types = ["implicit", "refresh_token", "authorization_code", "password", "client_credentials"]
response_types = ["code"] # use authorization code flow, see https://developer.okta.com/blog/2019/05/01/is-the-oauth-implicit-flow-dead for details
scopes = ["openid", "profile", "email", "offline"]
public = true # force PKCS for public clients

[http.services.oidcprovider.clients.reva]
id = "reva"
grant_types = ["implicit", "refresh_token", "authorization_code", "password", "client_credentials"]
response_types = ["code"] # use authorization code flow
# private clients can use a secret
client_secret = "$2a$10$IxMdI6d.LIRZPpSfEwNoeu4rY3FhDREsxFJXikcgdRRAStxUlsuEO"  # = "foobar"
scopes = ["openid", "profile", "email", "offline"]

# to debug the oidc provider allow https://oidcdebugger.com
[http.services.oidcprovider.clients.oidcdebugger]
id = "oidcdebugger"
redirect_uris = ["https://oidcdebugger.com/debug"]
grant_types = ["implicit", "refresh_token", "authorization_code", "password", "client_credentials"]
response_types = ["id_token token", "code"]
client_secret = "$2a$10$IxMdI6d.LIRZPpSfEwNoeu4rY3FhDREsxFJXikcgdRRAStxUlsuEO"  # = "foobar"
scopes = ["openid", "profile", "email", "offline"]

[http.services.ocdav]
# serve ocdav on the root path
prefix = ""
chunk_folder = "/var/tmp/revad/chunks"
# for user lookups
gateway = "localhost:19000"
# prefix the path of requests to /dav/files with this namespace
# While owncloud has only listed usernames at this endpoint CERN has
# been exposing more than just usernames. For owncloud deployments we
# can prefix the path to jail the requests to the correct CS3 namespace.
# In this deployment we mounted the owncloud storage provider at /oc. It
# expects a username as the first path segment.
files_namespace = "/"
# currently, only the desktop client will use this endpoint, but only if
# the dav.chunking capability is available
# TODO implement a path wrapper that rewrites `<username>` into the path
# layout for the users home?
# no, use GetHome?
# for eos we need to rewrite the path
# TODO strip the username from the path so the CS3 namespace can be mounted
# at the files/<username> endpoint? what about migration? separate reva instance

# similar to the dav/files endpoint we can configure a prefix for the old webdav endpoint
# we use the old webdav endpoint to present the cs3 namespace
webdav_namespace = "/"
# note: this changes the tree that is rendered at remote.php/webdav from the users home to the cs3 namespace
# use webdav_namespace = "/home" to use the old namespace that only exposes the users files
# this endpoint should not affect the desktop client sync but will present different folders for the other clients:
# - the desktop clients use a hardcoded remote.php/dav/files/<username> if the dav.chunkung capability is present
# - the ios ios uses the core.webdav-root capability which points to remote.php/webdav in oc10
# - the oc js sdk is hardcoded to the remote.php/webdav so it will see the new tree
# - TODO android? no sync ... but will see different tree

[http.services.ocs]
# prefix = "ocs"
# for user lookups and sharing
gateway = "localhost:19000"
